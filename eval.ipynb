{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "4a7aa0b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set your OpenAI API key (required for openai/gpt-4o-mini).\n",
        "# When you run this cell, a prompt will appear—paste your key there (it won't be shown on screen).\n",
        "import os\n",
        "import getpass\n",
        "\n",
        "if \"OPENAI_API_KEY\" not in os.environ:\n",
        "    key = getpass.getpass(\"OpenAI API key: \")\n",
        "    if key:\n",
        "        os.environ[\"OPENAI_API_KEY\"] = key\n",
        "    else:\n",
        "        print(\"No key entered. Set OPENAI_API_KEY in this notebook or in your shell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "19adcd9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/eddiej@nisos.com/mespace/dspy/dspy-venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import dspy\n",
        "\n",
        "lm = dspy.LM('openai/gpt-4o-mini')\n",
        "dspy.configure(lm=lm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "bdc895cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of how the kernel manages memory for processes.\n",
            "\n",
            "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel and can be used by processes without any special handling. In a 32-bit system, this is usually the first 896 MB of RAM (though this can vary based on the architecture and configuration). Low memory is used for kernel data structures and for user-space processes that require direct access to memory.\n",
            "\n",
            "- **High Memory**: This refers to memory that is above the low memory limit and is not directly accessible by the kernel in a 32-bit system. Processes can use this memory, but the kernel must use special mechanisms (like paging) to access it. High memory is often used in systems with large amounts of RAM, allowing more memory to be allocated to user-space processes while still maintaining a limited address space for the kernel.\n",
            "\n",
            "In summary, low memory is directly accessible by the kernel, while high memory requires additional handling for the kernel to access it, especially in 32-bit systems.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "qa = dspy.Predict('question: str -> response: str')\n",
        "response = qa(question=\"what are high memory and low memory on linux?\")\n",
        "\n",
        "print(response.response)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "715a4c35",
      "metadata": {},
      "source": [
        "You can inspect the n last prompts sent by DSPy easily. Alternatively, if you enabled MLflow Tracing above, you can see the full LLM interactions for each program execution in a tree view."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "c9ea2919",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-10T13:30:58.857597]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `question` (str):\n",
            "Your output fields are:\n",
            "1. `response` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Given the fields `question`, produce the fields `response`.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## question ## ]]\n",
            "what are high memory and low memory on linux?\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## response ## ]]\n",
            "In Linux, \"high memory\" and \"low memory\" refer to different regions of the system's memory address space, particularly in the context of how the kernel manages memory for processes.\n",
            "\n",
            "- **Low Memory**: This typically refers to the memory that is directly accessible by the kernel and can be used by processes without any special handling. In a 32-bit system, this is usually the first 896 MB of RAM (though this can vary based on the architecture and configuration). Low memory is used for kernel data structures and for user-space processes that require direct access to memory.\n",
            "\n",
            "- **High Memory**: This refers to memory that is above the low memory limit and is not directly accessible by the kernel in a 32-bit system. Processes can use this memory, but the kernel must use special mechanisms (like paging) to access it. High memory is often used in systems with large amounts of RAM, allowing more memory to be allocated to user-space processes while still maintaining a limited address space for the kernel.\n",
            "\n",
            "In summary, low memory is directly accessible by the kernel, while high memory requires additional handling for the kernel to access it, especially in 32-bit systems.\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dspy.inspect_history(n=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a44ce862",
      "metadata": {},
      "source": [
        "DSPy has various built-in modules, e.g. dspy.ChainOfThought, dspy.ProgramOfThought, and dspy.ReAct. These are interchangeable with basic dspy.Predict: they take your signature, which is specific to your task, and they apply general-purpose prompting techniques and inference-time strategies to it.\n",
        "\n",
        "For example, dspy.ChainOfThought is an easy way to elicit reasoning out of your LM before it commits to the outputs requested in your signature.\n",
        "\n",
        "In the example below, we'll omit str types (as the default type is string). You should feel free to experiment with other fields and types, e.g. try topics: list[str] or is_realistic: bool.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "02b07263",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    reasoning='The placement of curly braces on their own line is often a matter of coding style and conventions. In many programming languages, such as Java, C#, and JavaScript, it is common to place opening curly braces on the same line as the preceding statement, while closing curly braces may be placed on their own line. This style enhances readability and maintains a clean structure. However, some coding standards, like those used in Python or certain configurations of C++, may prefer braces to be on their own lines for clarity. Ultimately, it depends on the specific coding guidelines being followed.',\n",
              "    response=\"Curly braces do not necessarily have to appear on their own line; it depends on the coding style and conventions being used. Many developers prefer to keep opening braces on the same line as the preceding statement for readability, while closing braces are often placed on their own line. It's best to follow the coding standards of the project or language you are working with.\"\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# cot = dspy.ChainOfThought('question -> response: list[str]')\n",
        "# cot(question=\"should curly braces appear on their own line?\")\n",
        "\n",
        "# cot = dspy.ChainOfThought('question -> response: bool')\n",
        "# cot(question=\"should curly braces appear on their own line?\")\n",
        "\n",
        "cot = dspy.ChainOfThought('question -> response')\n",
        "cot(question=\"should curly braces appear on their own line?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "22e83618",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-10T13:36:09.478108]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `response` (bool):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}        # note: the value you produce must be True or False\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Given the fields `question`, produce the fields `response`.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## question ## ]]\n",
            "should curly braces appear on their own line?\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]` (must be formatted as a valid Python bool), and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The placement of curly braces on their own line is a matter of coding style and conventions. In some programming languages and style guides, such as those that follow the Allman style, it is common to place opening curly braces on a new line. In contrast, other styles, like K&R style, place the opening brace on the same line as the statement. Therefore, whether curly braces should appear on their own line depends on the specific coding standards being followed.\n",
            "\n",
            "[[ ## response ## ]]\n",
            "False\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-10T13:37:11.891442]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `response` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Given the fields `question`, produce the fields `response`.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## question ## ]]\n",
            "should curly braces appear on their own line?\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The placement of curly braces on their own line is often a matter of coding style and conventions. In many programming languages, such as Java, C#, and JavaScript, it is common to place opening curly braces on the same line as the preceding statement, while closing curly braces may be placed on their own line. This style enhances readability and maintains a clean structure. However, some coding standards, like those used in Python or certain configurations of C++, may prefer braces to be on their own lines for clarity. Ultimately, it depends on the specific coding guidelines being followed.\n",
            "\n",
            "[[ ## response ## ]]\n",
            "Curly braces do not necessarily have to appear on their own line; it depends on the coding style and conventions being used. Many developers prefer to keep opening braces on the same line as the preceding statement for readability, while closing braces are often placed on their own line. It's best to follow the coding standards of the project or language you are working with.\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dspy.inspect_history(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8b790fc9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data file: /Users/eddiej@nisos.com/mespace/dspy/ragqa_arena_tech_examples.jsonl\n"
          ]
        }
      ],
      "source": [
        "import orjson\n",
        "import requests\n",
        "from pathlib import Path\n",
        "\n",
        "# Save in this repo so the path is predictable (run this cell from workspace/dspy or set path below)\n",
        "DATA_PATH = Path(\"ragqa_arena_tech_examples.jsonl\").resolve()\n",
        "print(f\"Data file: {DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9613b8b5",
      "metadata": {},
      "source": [
        "Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf1b3d74",
      "metadata": {},
      "outputs": [],
      "source": [
        "URL = \"https://huggingface.co/dspy/cache/resolve/main/ragqa_arena_tech_examples.jsonl\"\n",
        "if not DATA_PATH.exists():\n",
        "    print(f\"Downloading to {DATA_PATH}...\")\n",
        "    r = requests.get(URL, stream=True)\n",
        "    r.raise_for_status()\n",
        "    with open(DATA_PATH, \"wb\") as f:\n",
        "        for chunk in r.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "else:\n",
        "    print(f\"Using existing file: {DATA_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ad0e4c66",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'question': 'why igp is used in mpls?',\n",
              " 'response': \"An IGP exchanges routing prefixes between gateways/routers.  \\nWithout a routing protocol, you'd have to configure each route on every router and you'd have no dynamic updates when routes change because of link failures. \\nFuthermore, within an MPLS network, an IGP is vital for advertising the internal topology and ensuring connectivity for MP-BGP inside the network.\",\n",
              " 'gold_doc_ids': [2822, 2823]}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open(DATA_PATH) as f:\n",
        "    data = [orjson.loads(line) for line in f]\n",
        "    \n",
        "data[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "94644a72",
      "metadata": {},
      "outputs": [],
      "source": [
        "import dspy\n",
        "data = [dspy.Example(**d).with_inputs('question') for d in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "93e24ea3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(200, 300, 500)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.Random(0).shuffle(data)\n",
        "trainset, devset, testset = data[:200], data[200:500], data[500:1000]\n",
        "\n",
        "len(trainset), len(devset), len(testset)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a2f6bde9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Example({'question': 'how create a temporary file in shell script?', 'response': 'Use mktemp to create a temporary file `temp_file=$(mktemp)` or, alternatively, to create a temporary directory: `temp_dir=$(mktemp -d)`. \\nAt the end of the script you have to delete the temporary file or directory `rm ${temp_file} rm -R ${temp_dir} mktemp`.', 'gold_doc_ids': [4276]}) (input_keys={'question'})"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = data[4]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "de2a9444",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question: \t how create a temporary file in shell script?\n",
            "\n",
            "Gold Response: \t Use mktemp to create a temporary file `temp_file=$(mktemp)` or, alternatively, to create a temporary directory: `temp_dir=$(mktemp -d)`. \n",
            "At the end of the script you have to delete the temporary file or directory `rm ${temp_file} rm -R ${temp_dir} mktemp`.\n",
            "\n",
            "Predicted Response: \t You can create a temporary file in a shell script using the following command:\n",
            "\n",
            "```bash\n",
            "temp_file=$(mktemp /tmp/mytempfile.XXXXXX)\n",
            "```\n",
            "\n",
            "This command creates a temporary file in the `/tmp` directory with a name that starts with `mytempfile.` followed by six random characters. You can then use `$temp_file` to refer to the temporary file in your script. Remember to clean up the temporary file after use by removing it with `rm $temp_file`.\n",
            "\n",
            "Semantic F1 Score: 0.67\n"
          ]
        }
      ],
      "source": [
        "from dspy.evaluate import SemanticF1\n",
        "\n",
        "# Instantiate the metric.\n",
        "metric = SemanticF1(decompositional=True)\n",
        "\n",
        "# Produce a prediction from our `cot` module, using the `example` above as input.\n",
        "pred = cot(**example.inputs())\n",
        "\n",
        "# Compute the metric score for the prediction.\n",
        "score = metric(example, pred)\n",
        "\n",
        "print(f\"Question: \\t {example.question}\\n\")\n",
        "print(f\"Gold Response: \\t {example.response}\\n\")\n",
        "print(f\"Predicted Response: \\t {pred.response}\\n\")\n",
        "print(f\"Semantic F1 Score: {score:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "a3657d38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-10T13:56:01.591461]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `question` (str): \n",
            "2. `ground_truth` (str): \n",
            "3. `system_response` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `ground_truth_key_ideas` (str): enumeration of key ideas in the ground truth\n",
            "3. `system_response_key_ideas` (str): enumeration of key ideas in the system response\n",
            "4. `discussion` (str): discussion of the overlap between ground truth and system response\n",
            "5. `recall` (float): fraction (out of 1.0) of ground truth covered by the system response\n",
            "6. `precision` (float): fraction (out of 1.0) of system response covered by the ground truth\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## ground_truth ## ]]\n",
            "{ground_truth}\n",
            "\n",
            "[[ ## system_response ## ]]\n",
            "{system_response}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## ground_truth_key_ideas ## ]]\n",
            "{ground_truth_key_ideas}\n",
            "\n",
            "[[ ## system_response_key_ideas ## ]]\n",
            "{system_response_key_ideas}\n",
            "\n",
            "[[ ## discussion ## ]]\n",
            "{discussion}\n",
            "\n",
            "[[ ## recall ## ]]\n",
            "{recall}        # note: the value you produce must be a single float value\n",
            "\n",
            "[[ ## precision ## ]]\n",
            "{precision}        # note: the value you produce must be a single float value\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Compare a system's response to the ground truth to compute recall and precision of key ideas.\n",
            "        You will first enumerate key ideas in each response, discuss their overlap, and then report recall and precision.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## question ## ]]\n",
            "how create a temporary file in shell script?\n",
            "\n",
            "[[ ## ground_truth ## ]]\n",
            "Use mktemp to create a temporary file `temp_file=$(mktemp)` or, alternatively, to create a temporary directory: `temp_dir=$(mktemp -d)`. \n",
            "At the end of the script you have to delete the temporary file or directory `rm ${temp_file} rm -R ${temp_dir} mktemp`.\n",
            "\n",
            "[[ ## system_response ## ]]\n",
            "You can create a temporary file in a shell script using the following command:\n",
            "\n",
            "```bash\n",
            "temp_file=$(mktemp /tmp/mytempfile.XXXXXX)\n",
            "```\n",
            "\n",
            "This command creates a temporary file in the `/tmp` directory with a name that starts with `mytempfile.` followed by six random characters. You can then use `$temp_file` to refer to the temporary file in your script. Remember to clean up the temporary file after use by removing it with `rm $temp_file`.\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## ground_truth_key_ideas ## ]]`, then `[[ ## system_response_key_ideas ## ]]`, then `[[ ## discussion ## ]]`, then `[[ ## recall ## ]]` (must be formatted as a valid Python float), then `[[ ## precision ## ]]` (must be formatted as a valid Python float), and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The system response provides a method to create a temporary file using `mktemp`, which is a key idea present in the ground truth. It also mentions the importance of cleaning up the temporary file after use, which aligns with the ground truth's instruction to delete the temporary file. However, the system response does not mention the alternative of creating a temporary directory or the specific command to delete it. Overall, the system response captures the essence of the ground truth but lacks some details.\n",
            "\n",
            "[[ ## ground_truth_key_ideas ## ]]\n",
            "1. Use `mktemp` to create a temporary file.\n",
            "2. Example command: `temp_file=$(mktemp)`.\n",
            "3. Alternative command to create a temporary directory: `temp_dir=$(mktemp -d)`.\n",
            "4. Clean up by deleting the temporary file or directory: `rm ${temp_file}` and `rm -R ${temp_dir}`.\n",
            "\n",
            "[[ ## system_response_key_ideas ## ]]\n",
            "1. Create a temporary file using `mktemp`.\n",
            "2. Example command: `temp_file=$(mktemp /tmp/mytempfile.XXXXXX)`.\n",
            "3. Clean up the temporary file after use with `rm $temp_file`.\n",
            "\n",
            "[[ ## discussion ## ]]\n",
            "The system response includes the key idea of using `mktemp` to create a temporary file and provides a specific example of how to do so, which is a positive overlap with the ground truth. It also emphasizes the need to clean up the temporary file, which is another important aspect mentioned in the ground truth. However, it does not cover the alternative of creating a temporary directory or the specific command for deleting it, which limits its completeness. Therefore, while there is significant overlap, some key ideas from the ground truth are missing.\n",
            "\n",
            "[[ ## recall ## ]]\n",
            "0.75\n",
            "\n",
            "[[ ## precision ## ]]\n",
            "0.6\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dspy.inspect_history(n=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "c01851cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 125.66 / 300 (41.9%): 100%|██████████| 300/300 [00:00<00:00, 480.16it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 11:53:01 INFO dspy.evaluate.evaluate: Average Metric: 125.65754569486975 / 300 (41.9%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>example_response</th>\n",
              "      <th>gold_doc_ids</th>\n",
              "      <th>reasoning</th>\n",
              "      <th>pred_response</th>\n",
              "      <th>SemanticF1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when to use c over c++, and c++ over c?</td>\n",
              "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
              "      <td>[733]</td>\n",
              "      <td>C and C++ are both powerful programming languages, but they serve ...</td>\n",
              "      <td>Use C when you need low-level system programming, performance, and...</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>should images be stored in a git repository?</td>\n",
              "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
              "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
              "      <td>Storing images in a Git repository can be problematic for several ...</td>\n",
              "      <td>Images should generally not be stored in a Git repository due to i...</td>\n",
              "      <td>✔️ [0.333]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       question  \\\n",
              "0       when to use c over c++, and c++ over c?   \n",
              "1  should images be stored in a git repository?   \n",
              "\n",
              "                                                        example_response  \\\n",
              "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
              "1  One viewpoint expresses that there is no significant downside, esp...   \n",
              "\n",
              "                     gold_doc_ids  \\\n",
              "0                           [733]   \n",
              "1  [6253, 6254, 6275, 6278, 8215]   \n",
              "\n",
              "                                                               reasoning  \\\n",
              "0  C and C++ are both powerful programming languages, but they serve ...   \n",
              "1  Storing images in a Git repository can be problematic for several ...   \n",
              "\n",
              "                                                           pred_response  \\\n",
              "0  Use C when you need low-level system programming, performance, and...   \n",
              "1  Images should generally not be stored in a Git repository due to i...   \n",
              "\n",
              "   SemanticF1  \n",
              "0              \n",
              "1  ✔️ [0.333]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                text-align: center;\n",
              "                font-size: 16px;\n",
              "                font-weight: bold;\n",
              "                color: #555;\n",
              "                margin: 10px 0;'>\n",
              "                ... 298 more rows not displayed ...\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "41.89"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define an evaluator that we can re-use.\n",
        "evaluate = dspy.Evaluate(devset=devset, metric=metric, num_threads=24,\n",
        "                         display_progress=True, display_table=2)\n",
        "\n",
        "# Evaluate the Chain-of-Thought program.\n",
        "evaluate(cot)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53175c1c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c4f7b5e0",
      "metadata": {},
      "source": [
        "Advance it with RAG"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a453c9f2",
      "metadata": {},
      "source": [
        "Let's Build a RAG and see if this performs better"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5dad5c9",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "01ab529c",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 28436 documents. Will encode them below.\n",
            "Training a 32-byte FAISS index with 337 partitions, based on 28436 x 512-dim embeddings\n"
          ]
        }
      ],
      "source": [
        "max_characters = 6000  # for truncating >99th percentile of documents\n",
        "topk_docs_to_retrieve = 5  # number of documents to retrieve per search query\n",
        "\n",
        "with open(\"ragqa_arena_tech_corpus.jsonl\") as f:\n",
        "    corpus = [orjson.loads(line)['text'][:max_characters] for line in f]\n",
        "    print(f\"Loaded {len(corpus)} documents. Will encode them below.\")\n",
        "\n",
        "embedder = dspy.Embedder('openai/text-embedding-3-small', dimensions=512)\n",
        "search = dspy.retrievers.Embeddings(embedder=embedder, corpus=corpus, k=topk_docs_to_retrieve)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c2addf65",
      "metadata": {},
      "outputs": [],
      "source": [
        "class RAG(dspy.Module):\n",
        "    def __init__(self):\n",
        "        self.respond = dspy.ChainOfThought('context, question -> response')\n",
        "\n",
        "    def forward(self, question):\n",
        "        context = search(question).passages\n",
        "        return self.respond(context=context, question=question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "ef30a97c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    reasoning=\"High Memory and Low Memory are two segments of memory management in the Linux kernel, particularly relevant in 32-bit architectures. Low Memory refers to the portion of memory that the kernel can access directly, which is always mapped in the kernel's address space. This allows the kernel to perform operations without needing to map memory pages explicitly. High Memory, on the other hand, is a segment of memory that is not permanently mapped in the kernel's address space. When the kernel needs to access High Memory, it must temporarily map it into its address space using functions like `kmap`. This distinction is crucial for managing memory efficiently, especially when dealing with large amounts of RAM, as it allows the kernel to handle more memory than what is directly addressable in its space.\",\n",
              "    response=\"In Linux, High Memory refers to the segment of memory that is not permanently mapped into the kernel's address space, requiring the kernel to use functions like `kmap` to access it. Low Memory is the portion of memory that the kernel can access directly and is always mapped in the kernel's address space. This separation helps manage memory efficiently, especially in 32-bit systems where the kernel needs to handle more memory than it can directly address.\"\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag = RAG()\n",
        "rag(question=\"what are high memory and low memory on linux?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "1ad18b63",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 170.06 / 300 (56.7%): 100%|██████████| 300/300 [03:00<00:00,  1.66it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 11:56:10 INFO dspy.evaluate.evaluate: Average Metric: 170.05686452754802 / 300 (56.7%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>example_response</th>\n",
              "      <th>gold_doc_ids</th>\n",
              "      <th>reasoning</th>\n",
              "      <th>pred_response</th>\n",
              "      <th>SemanticF1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when to use c over c++, and c++ over c?</td>\n",
              "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
              "      <td>[733]</td>\n",
              "      <td>C should be used over C++ primarily in scenarios where simplicity ...</td>\n",
              "      <td>Use C over C++ when working on embedded systems, requiring low-lev...</td>\n",
              "      <td>✔️ [0.500]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>should images be stored in a git repository?</td>\n",
              "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
              "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
              "      <td>Storing images in a Git repository can be problematic due to Git's...</td>\n",
              "      <td>While it is technically possible to store images in a Git reposito...</td>\n",
              "      <td>✔️ [0.500]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       question  \\\n",
              "0       when to use c over c++, and c++ over c?   \n",
              "1  should images be stored in a git repository?   \n",
              "\n",
              "                                                        example_response  \\\n",
              "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
              "1  One viewpoint expresses that there is no significant downside, esp...   \n",
              "\n",
              "                     gold_doc_ids  \\\n",
              "0                           [733]   \n",
              "1  [6253, 6254, 6275, 6278, 8215]   \n",
              "\n",
              "                                                               reasoning  \\\n",
              "0  C should be used over C++ primarily in scenarios where simplicity ...   \n",
              "1  Storing images in a Git repository can be problematic due to Git's...   \n",
              "\n",
              "                                                           pred_response  \\\n",
              "0  Use C over C++ when working on embedded systems, requiring low-lev...   \n",
              "1  While it is technically possible to store images in a Git reposito...   \n",
              "\n",
              "   SemanticF1  \n",
              "0  ✔️ [0.500]  \n",
              "1  ✔️ [0.500]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                text-align: center;\n",
              "                font-size: 16px;\n",
              "                font-weight: bold;\n",
              "                color: #555;\n",
              "                margin: 10px 0;'>\n",
              "                ... 298 more rows not displayed ...\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "56.69"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(RAG())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dded7b6",
      "metadata": {},
      "source": [
        "Off the shelf, our RAG module scores 56.7%\n",
        "!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72cb89f0",
      "metadata": {},
      "source": [
        "Next, I will show you a further enhancement on the quality of the system provided by the signature of DSPy -> Prompt optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cedb553",
      "metadata": {},
      "source": [
        "Optimise self.respond = dspy.ChainOfThought('context, question -> response')\n",
        "However,if there are many sub-modules in your program, all of them will be optimized together."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfd0850b",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c90d3b36",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:02:47 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "RUNNING WITH THE FOLLOWING MEDIUM AUTO RUN SETTINGS:\n",
            "num_trials: 18\n",
            "minibatch: True\n",
            "num_fewshot_candidates: 12\n",
            "num_instruct_candidates: 6\n",
            "valset size: 160\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[93m\u001b[1mProjected Language Model (LM) Calls\u001b[0m\n",
            "\n",
            "Based on the parameters you have set, the maximum number of LM calls is projected as follows:\n",
            "\n",
            "\u001b[93m- Prompt Generation: \u001b[94m\u001b[1m10\u001b[0m\u001b[93m data summarizer calls + \u001b[94m\u001b[1m6\u001b[0m\u001b[93m * \u001b[94m\u001b[1m1\u001b[0m\u001b[93m lm calls in program + (\u001b[94m\u001b[1m2\u001b[0m\u001b[93m) lm calls in program-aware proposer = \u001b[94m\u001b[1m18\u001b[0m\u001b[93m prompt model calls\u001b[0m\n",
            "\u001b[93m- Program Evaluation: \u001b[94m\u001b[1m35\u001b[0m\u001b[93m examples in minibatch * \u001b[94m\u001b[1m18\u001b[0m\u001b[93m batches + \u001b[94m\u001b[1m160\u001b[0m\u001b[93m examples in val set * \u001b[94m\u001b[1m4\u001b[0m\u001b[93m full evals = \u001b[94m\u001b[1m1270\u001b[0m\u001b[93m LM Program calls\u001b[0m\n",
            "\n",
            "\u001b[93m\u001b[1mEstimated Cost Calculation:\u001b[0m\n",
            "\n",
            "\u001b[93mTotal Cost = (Number of calls to task model * (Avg Input Token Length per Call * Task Model Price per Input Token + Avg Output Token Length per Call * Task Model Price per Output Token)\n",
            "            + (Number of program calls * (Avg Input Token Length per Call * Task Prompt Price per Input Token + Avg Output Token Length per Call * Prompt Model Price per Output Token).\u001b[0m\n",
            "\n",
            "For a preliminary estimate of potential costs, we recommend you perform your own calculations based on the task\n",
            "and prompt models you intend to use. If the projected costs exceed your budget or expectations, you may consider:\n",
            "\n",
            "\u001b[93m- Reducing the number of trials (`num_trials`), the size of the valset, or the number of LM calls in your program.\u001b[0m\n",
            "\u001b[93m- Using a cheaper task model to optimize the prompt.\u001b[0m\n",
            "\u001b[93m- Setting `minibatch=True` if you haven't already.\u001b[0m\n",
            "\n",
            "To proceed with the execution of this program, please confirm by typing \u001b[94m'y'\u001b[0m for yes or \u001b[94m'n'\u001b[0m for no.\n",
            "If no input is received within 20 seconds, the program will proceed automatically.\n",
            "\n",
            "If you would like to bypass this confirmation step in future executions, set the \u001b[93m`requires_permission_to_run`\u001b[0m flag to \u001b[93m`False`\u001b[0m when calling compile.\n",
            "\n",
            "\u001b[93mAwaiting your input...\u001b[0m\n",
            "\n",
            "Do you wish to continue? (y/n): "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:03:08 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "==> STEP 1: BOOTSTRAP FEWSHOT EXAMPLES <==\n",
            "2026/02/13 12:03:08 INFO dspy.teleprompt.mipro_optimizer_v2: These will be used as few-shot example candidates for our program and for creating instructions.\n",
            "\n",
            "2026/02/13 12:03:08 INFO dspy.teleprompt.mipro_optimizer_v2: Bootstrapping N=12 sets of demonstrations...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "No input received within 20 seconds. Proceeding with execution...\n",
            "Bootstrapping set 1/12\n",
            "Bootstrapping set 2/12\n",
            "Bootstrapping set 3/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/40 [00:00<?, ?it/s]/Users/eddiej@nisos.com/mespace/dspy/dspy-venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "  5%|▌         | 2/40 [00:30<09:42, 15.33s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 4/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 15%|█▌        | 6/40 [01:27<08:13, 14.53s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 6 examples for up to 1 rounds, amounting to 6 attempts.\n",
            "Bootstrapping set 5/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:29<09:13, 14.55s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 6/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:41<13:13, 20.88s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 7/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:31<09:52, 15.58s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 8/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 4/40 [00:52<07:55, 13.21s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 4 examples for up to 1 rounds, amounting to 4 attempts.\n",
            "Bootstrapping set 9/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:30<09:48, 15.50s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 2 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 10/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:20<06:38, 10.47s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n",
            "Bootstrapping set 11/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▎         | 1/40 [00:12<08:09, 12.56s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 1 examples for up to 1 rounds, amounting to 1 attempts.\n",
            "Bootstrapping set 12/12\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  5%|▌         | 2/40 [00:25<08:01, 12.66s/it]\n",
            "2026/02/13 12:09:10 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "==> STEP 2: PROPOSE INSTRUCTION CANDIDATES <==\n",
            "2026/02/13 12:09:10 INFO dspy.teleprompt.mipro_optimizer_v2: We will use the few-shot examples from the previous step, a generated dataset summary, a summary of the program code, and a randomly selected prompting tip to propose instructions.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bootstrapped 1 full traces after 2 examples for up to 1 rounds, amounting to 2 attempts.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:09:36 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "Proposing N=6 instructions...\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: Proposed Instructions for Predictor 0:\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 0: Given the fields `context`, `question`, produce the fields `response`.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 1: In a high-stakes situation where you are facing a critical technical issue on macOS or Linux, you need to quickly troubleshoot and resolve the problem. Given the provided `context` that contains relevant troubleshooting information and the `question` detailing your specific issue, produce a detailed and coherent `response`. This response should guide the user through the necessary steps to solve the issue effectively, ensuring they can regain functionality without further delay.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 2: You are a technical support specialist assisting users with troubleshooting issues on macOS and command-line environments. Given the fields `context` that contains relevant troubleshooting information, and `question` that specifies the user's technical issue, produce a structured `response` that outlines step-by-step solutions or explanations derived from the context provided.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 3: Produce a structured response that answers the user's question based on the provided context. Ensure that the reasoning is clear and step-by-step, and include relevant details from the context to support the answer.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 4: Using the provided `context` and `question`, generate a detailed `response` that includes a clear reasoning process explaining how the answer was derived. Ensure that your response is informative and enhances the user's understanding of the topic.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: 5: Given the context provided, analyze the question and produce a well-reasoned response that not only answers the question but also includes the reasoning process that led to the conclusion.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: ==> STEP 3: FINDING OPTIMAL PROMPT PARAMETERS <==\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: We will evaluate the program over a series of trials with different combinations of instructions and few-shot examples to find the optimal combination using Bayesian Optimization.\n",
            "\n",
            "2026/02/13 12:10:29 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 1 / 23 - Full Evaluation of Default Program ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 88.81 / 160 (55.5%): 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:11:57 INFO dspy.evaluate.evaluate: Average Metric: 88.81023055967225 / 160 (55.5%)\n",
            "2026/02/13 12:11:57 INFO dspy.teleprompt.mipro_optimizer_v2: Default program score: 55.51\n",
            "\n",
            "/Users/eddiej@nisos.com/mespace/dspy/dspy-venv/lib/python3.9/site-packages/optuna/_experimental.py:33: ExperimentalWarning: Argument ``multivariate`` is an experimental feature. The interface can change in the future.\n",
            "  optuna_warn(\n",
            "2026/02/13 12:11:57 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 2 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 21.51 / 35 (61.4%): 100%|██████████| 35/35 [00:26<00:00,  1.31it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:12:24 INFO dspy.evaluate.evaluate: Average Metric: 21.50658845868181 / 35 (61.4%)\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.45 on minibatch of size 35 with parameters ['Predictor 0: Instruction 1', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45]\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51]\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.51\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:12:24 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 3 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 19.48 / 35 (55.6%): 100%|██████████| 35/35 [00:22<00:00,  1.58it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:12:46 INFO dspy.evaluate.evaluate: Average Metric: 19.47621983861602 / 35 (55.6%)\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.65 on minibatch of size 35 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 2'].\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65]\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51]\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.51\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:12:46 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 4 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 22.35 / 35 (63.9%): 100%|██████████| 35/35 [00:26<00:00,  1.34it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:13:12 INFO dspy.evaluate.evaluate: Average Metric: 22.354205752328923 / 35 (63.9%)\n",
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 63.87 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87]\n",
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51]\n",
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.51\n",
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:13:13 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 5 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 21.16 / 35 (60.4%): 100%|██████████| 35/35 [00:30<00:00,  1.13it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:13:44 INFO dspy.evaluate.evaluate: Average Metric: 21.157303053486697 / 35 (60.4%)\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 60.45 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 4'].\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45]\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51]\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.51\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:13:44 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 6 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 19.62 / 35 (56.1%): 100%|██████████| 35/35 [00:29<00:00,  1.18it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:14:13 INFO dspy.evaluate.evaluate: Average Metric: 19.623056895603526 / 35 (56.1%)\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 56.07 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 5'].\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07]\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51]\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 55.51\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 7 / 23 - Full Evaluation =====\n",
            "2026/02/13 12:14:13 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 63.87) from minibatch trials...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 95.27 / 160 (59.5%): 100%|██████████| 160/160 [01:27<00:00,  1.83it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:15:41 INFO dspy.evaluate.evaluate: Average Metric: 95.27402803389197 / 160 (59.5%)\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 59.55\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2026/02/13 12:15:41 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 8 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 22.00 / 35 (62.8%): 100%|██████████| 35/35 [00:23<00:00,  1.48it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:16:05 INFO dspy.evaluate.evaluate: Average Metric: 21.99591415854313 / 35 (62.8%)\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.85 on minibatch of size 35 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85]\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:16:05 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 9 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 20.56 / 35 (58.7%): 100%|██████████| 35/35 [00:30<00:00,  1.17it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:16:35 INFO dspy.evaluate.evaluate: Average Metric: 20.55897267808781 / 35 (58.7%)\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 58.74 on minibatch of size 35 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 1'].\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74]\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: =========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:16:35 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 10 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 19.36 / 35 (55.3%): 100%|██████████| 35/35 [00:24<00:00,  1.41it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:17:00 INFO dspy.evaluate.evaluate: Average Metric: 19.363254139344495 / 35 (55.3%)\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 55.32 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 3'].\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32]\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:17:00 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 11 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 20.12 / 35 (57.5%): 100%|██████████| 35/35 [00:37<00:00,  1.06s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:17:37 INFO dspy.evaluate.evaluate: Average Metric: 20.12047252118904 / 35 (57.5%)\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.49 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 9'].\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49]\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:17:37 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 12 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 21.96 / 35 (62.7%): 100%|██████████| 35/35 [00:25<00:00,  1.36it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:18:03 INFO dspy.evaluate.evaluate: Average Metric: 21.955208844747204 / 35 (62.7%)\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 62.73 on minibatch of size 35 with parameters ['Predictor 0: Instruction 4', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73]\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55]\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 59.55\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 13 / 23 - Full Evaluation =====\n",
            "2026/02/13 12:18:03 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 62.79) from minibatch trials...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 96.46 / 160 (60.3%): 100%|██████████| 160/160 [01:16<00:00,  2.10it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:19:19 INFO dspy.evaluate.evaluate: Average Metric: 96.46223639410225 / 160 (60.3%)\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: \u001b[92mNew best full eval score!\u001b[0m Score: 60.29\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2026/02/13 12:19:19 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 14 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 19.95 / 35 (57.0%): 100%|██████████| 35/35 [00:33<00:00,  1.06it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:19:52 INFO dspy.evaluate.evaluate: Average Metric: 19.950705531189197 / 35 (57.0%)\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 57.0 on minibatch of size 35 with parameters ['Predictor 0: Instruction 3', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0]\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:19:52 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 15 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 22.51 / 35 (64.3%): 100%|██████████| 35/35 [00:41<00:00,  1.18s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:20:33 INFO dspy.evaluate.evaluate: Average Metric: 22.512206781207883 / 35 (64.3%)\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 64.32 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32]\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:20:33 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 16 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 22.52 / 35 (64.3%): 100%|██████████| 35/35 [00:34<00:00,  1.00it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:21:08 INFO dspy.evaluate.evaluate: Average Metric: 22.519941063221182 / 35 (64.3%)\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 64.34 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34]\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:21:08 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 17 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 21.36 / 35 (61.0%): 100%|██████████| 35/35 [00:38<00:00,  1.10s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:21:47 INFO dspy.evaluate.evaluate: Average Metric: 21.36156748904507 / 35 (61.0%)\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.03 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34, 61.03]\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:21:47 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 18 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 21.67 / 35 (61.9%): 100%|██████████| 35/35 [00:39<00:00,  1.14s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:22:27 INFO dspy.evaluate.evaluate: Average Metric: 21.673829269341656 / 35 (61.9%)\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 61.93 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 10'].\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34, 61.03, 61.93]\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29]\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 19 / 23 - Full Evaluation =====\n",
            "2026/02/13 12:22:27 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 63.23) from minibatch trials...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 96.17 / 160 (60.1%): 100%|██████████| 160/160 [01:14<00:00,  2.15it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:23:41 INFO dspy.evaluate.evaluate: Average Metric: 96.17359005513732 / 160 (60.1%)\n",
            "2026/02/13 12:23:41 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29, 60.11]\n",
            "2026/02/13 12:23:41 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:23:41 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "2026/02/13 12:23:41 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2026/02/13 12:23:41 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 20 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 20.39 / 35 (58.2%): 100%|██████████| 35/35 [00:41<00:00,  1.19s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:24:23 INFO dspy.evaluate.evaluate: Average Metric: 20.386243699426224 / 35 (58.2%)\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 58.25 on minibatch of size 35 with parameters ['Predictor 0: Instruction 2', 'Predictor 0: Few-Shot Set 7'].\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34, 61.03, 61.93, 58.25]\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29, 60.11]\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:24:23 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 21 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 18.92 / 35 (54.1%): 100%|██████████| 35/35 [00:39<00:00,  1.12s/it]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:25:02 INFO dspy.evaluate.evaluate: Average Metric: 18.91922188774477 / 35 (54.1%)\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 54.05 on minibatch of size 35 with parameters ['Predictor 0: Instruction 5', 'Predictor 0: Few-Shot Set 11'].\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34, 61.03, 61.93, 58.25, 54.05]\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29, 60.11]\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:25:02 INFO dspy.teleprompt.mipro_optimizer_v2: == Trial 22 / 23 - Minibatch ==\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 19.77 / 35 (56.5%): 100%|██████████| 35/35 [00:01<00:00, 26.81it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:25:04 INFO dspy.evaluate.evaluate: Average Metric: 19.77398428662599 / 35 (56.5%)\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: Score: 56.5 on minibatch of size 35 with parameters ['Predictor 0: Instruction 0', 'Predictor 0: Few-Shot Set 6'].\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: Minibatch scores so far: [61.45, 55.65, 63.87, 60.45, 56.07, 62.85, 58.74, 55.32, 57.49, 62.73, 57.0, 64.32, 64.34, 61.03, 61.93, 58.25, 54.05, 56.5]\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29, 60.11]\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: ==========================================\n",
            "\n",
            "\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: ===== Trial 23 / 23 - Full Evaluation =====\n",
            "2026/02/13 12:25:04 INFO dspy.teleprompt.mipro_optimizer_v2: Doing full eval on next top averaging program (Avg Score: 61.93) from minibatch trials...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Average Metric: 96.19 / 160 (60.1%): 100%|██████████| 160/160 [01:30<00:00,  1.76it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 12:26:35 INFO dspy.evaluate.evaluate: Average Metric: 96.19174065637468 / 160 (60.1%)\n",
            "2026/02/13 12:26:35 INFO dspy.teleprompt.mipro_optimizer_v2: Full eval scores so far: [55.51, 59.55, 60.29, 60.11, 60.12]\n",
            "2026/02/13 12:26:35 INFO dspy.teleprompt.mipro_optimizer_v2: Best full score so far: 60.29\n",
            "2026/02/13 12:26:35 INFO dspy.teleprompt.mipro_optimizer_v2: =======================\n",
            "2026/02/13 12:26:35 INFO dspy.teleprompt.mipro_optimizer_v2: \n",
            "\n",
            "2026/02/13 12:26:35 INFO dspy.teleprompt.mipro_optimizer_v2: Returning best identified program with score 60.29!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "tp = dspy.MIPROv2(metric=metric, auto=\"medium\", num_threads=24)  # use fewer threads if your rate limit is small\n",
        "\n",
        "optimized_rag = tp.compile(RAG(), trainset=trainset,\n",
        "                           max_bootstrapped_demos=2, max_labeled_demos=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "71e927d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You are correct that cmd+tab does not work on hidden or minimized windows. It is primarily used to switch between active applications. To manage minimized windows, you may need to use other key combinations or adjust your system settings.\n"
          ]
        }
      ],
      "source": [
        "baseline = rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
        "print(baseline.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "11c98ce6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The Command + Tab functionality on Mac does not work with hidden or minimized windows directly. When you use Command + Tab, it allows you to cycle through your most recently used applications, but if an application is minimized, you cannot switch back to it using this shortcut unless you first switch to another application and let it take focus. This means that minimized windows will not be activated by Command + Tab until they are restored manually or until you switch focus away from them. If you want to manage minimized windows, you may need to use other shortcuts or methods, such as Command + Option + H + M to hide others and minimize the most recent item.\n"
          ]
        }
      ],
      "source": [
        "pred = optimized_rag(question=\"cmd+tab does not work on hidden or minimized windows\")\n",
        "print(pred.response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "72cf416b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-13T13:47:25.196387]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `context` (str): \n",
            "2. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `response` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## context ## ]]\n",
            "{context}\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Given the fields `context`, `question`, produce the fields `response`.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "[1] «If you hold command and quickly tap tab you will cycle between your two most recently used applications without bringing up the heads up display. If you want to use a third party app Witch has options to ignore minimized windows.»\n",
            "[2] «Try this: On your Mac, Navigate to System Preferences Go to Mission Control Uncheck When switching to an application, switch to a Space with open windows for the application Try using the cmd+tab now.»\n",
            "[3] «Command + Tab until you get the apps icon. Before releasing the Command key, press and hold the Option key. You must switch to another app and let it take focus first. In other words, you cant just Command + Tab to another app and before actually selecting that app (by releasing the Command and Tab keys), switch right back to your minimized app, which you might attempt to do if you minimized it by accident or just simply changed your mind shortly after minimizing. Both the Command and left Option keys must be pressed on the same side (left or right) of the keyboard.»\n",
            "[4] «Command-option-click does not hide all open windows. As stated in the linked Apple article, this command hides all other open items leaving the clicked item visible. It is possible to hide all other items and minimize your most recently item with command+option+h+m. This is a combination of command+option+h - Hide Others and command+option+m - Minimize.»\n",
            "[5] «Cmd+` of course will work but I do it in different way. I do the Cmd+tab when i do that, I dont release the Cmd so the opened apps are still visible. While it is visible, I will press ▼. This way it will show all your opened application with animated displayed so you can navigate using arrow keys.»\n",
            "\n",
            "[[ ## question ## ]]\n",
            "cmd+tab does not work on hidden or minimized windows\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The context provided explains various functionalities of the Command + Tab shortcut on a Mac, particularly in relation to switching between applications. It mentions that Command + Tab cycles through the most recently used applications but does not bring up minimized or hidden windows directly. To interact with minimized windows, users may need to adjust settings in System Preferences or use specific key combinations to manage window visibility. Therefore, the statement that \"cmd+tab does not work on hidden or minimized windows\" is accurate, as the shortcut is designed to switch between active applications rather than restore minimized ones.\n",
            "\n",
            "[[ ## response ## ]]\n",
            "You are correct that cmd+tab does not work on hidden or minimized windows. It is primarily used to switch between active applications. To manage minimized windows, you may need to use other key combinations or adjust your system settings.\n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\u001b[34m[2026-02-13T13:47:30.771186]\u001b[0m\n",
            "\n",
            "\u001b[31mSystem message:\u001b[0m\n",
            "\n",
            "Your input fields are:\n",
            "1. `context` (str): \n",
            "2. `question` (str):\n",
            "Your output fields are:\n",
            "1. `reasoning` (str): \n",
            "2. `response` (str):\n",
            "All interactions will be structured in the following way, with the appropriate values filled in.\n",
            "\n",
            "[[ ## context ## ]]\n",
            "{context}\n",
            "\n",
            "[[ ## question ## ]]\n",
            "{question}\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "{reasoning}\n",
            "\n",
            "[[ ## response ## ]]\n",
            "{response}\n",
            "\n",
            "[[ ## completed ## ]]\n",
            "In adhering to this structure, your objective is: \n",
            "        Using the provided `context` and `question`, generate a detailed `response` that includes a clear reasoning process explaining how the answer was derived. Ensure that your response is informative and enhances the user's understanding of the topic.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "This is an example of the task, though some input or output fields are not supplied.\n",
            "\n",
            "[[ ## question ## ]]\n",
            "locate vs find: usage, pros and cons of each other\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "Not supplied for this particular example. \n",
            "\n",
            "[[ ## response ## ]]\n",
            "Locate uses a prebuilt database, which should be regularly updated, while find iterates over a filesystem to locate files; locate has one big advantage over find: speed, but it can be inaccurate if the database has not been updated. \n",
            "\n",
            "Locate typically lists all files matching your pattern, leaving it to grep filtering and similar to cut this down to size, while find can do things to files it finds, one such operator being -exec.  \n",
            "\n",
            "Find is not intuitive for a novice or occasional user of Unix, and, historically, some versions didn't have the -print option as the default; locate is less flexible, but far more intuitive to use in the common case.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "[1] «The self-documenting code best practice is not a mainstream consensus - while it is widely acceptable that easy-to-understand code is better than cryptic code, not everyone agrees on whether or not complicated code must always be refactored or if its OK to comment it. But this debate is about pieces of code that are difficult to understand - and you are talking about commenting each and every line of code. Lines of code that are difficult to understand should be very rare - if most of your lines are complicated like this than you are overusing smart-ass one-liners and you should stop! Sure, a lines role may be a bit hard to understand sometimes, but thats its role in the context of a bigger piece of code - what the line itself does is almost always straightforward. So, you should not be commenting lines - you should be commenting pieces of code. Specific comments may be placed near the lines they refer to, but these comments are still referring to the bigger code. They do not describe what/why the line does - they describe what it does in that code and/or why is it needed in that code. So, it is not lines that should be commented but bigger pieces of code. Should every piece of code be commented? No. Harold Abelson said that Programs must be written for people to read, and only incidentally for machines to execute. But comments are only for people to read - the machine does not execute them. If your coding standards force writing redundant comments on every line/piece-of-code you are not just burdening the developer who needs to write them - you are also burdening the developers who will have to read them! This is a problem, because when most of the comments you read are redundant, you stop paying attention to them(because you know theyll just be redundant junk), and then youll miss out the important comments. So I say - write only the important comment, so that when someone see a comment in the code theyll know its worth to read it in order to understand the code.»\n",
            "[2] «Comment every line of code? No. The purpose of the others rules you talk about is precisely to avoid that. Comments to a readable code are at best redundant, and at worst will throw a reader off looking for the non-existent purpose of the comment. If you comment whole code which is self-explanatory, you double the amount of reading with out giving any additional information. I am not sure if such redundancy would pay off. One can imagine a reviewer saying that 42 says display_error while the comment says displays warning. But imagine a change in the code. You have two places to correct now. It becomes clear this style has copy-paste negatives. I would say that optimally the code should not need any comments, other than documentation. There are styles which go total opposite, if you have doubts about line its either: The code is too complicated and should be refactored into a function with a name with a semantic meaning for the portion of code. Be it a complex if or a portion of an algorithm. (Or a clever LINQ one-liner) If it cannot be simplified, you dont know enough idioms of language. (I personally am not a fanatic of strict no-comments rule, but I find it as a good initial mindset to go by.) Given all this is it even possible to write good coding standards that capture this idea? As for the process. Our standard gave quite much credit to the reviewer. Assuming they are not malicious this works good. When he asks What is variable o?, you rename it. If he asks what does this block do, either refactor or comment. If there was a debate whether something is clear or not, if the reviewer did not get it, then by definition it is not. On very rare occasions there was something like 2nd opinion from few friends. On average, you should get a code understandable by the average programmer in the team. IMO it keeps off the redundant information, and ensures that the code is understandable by at least one other member of the team, which we found a good optimum. Also, there were no absolutes. Though we were a small group. Its easy to find consensus in group of 5.»\n",
            "[3] «I dont see a compelling argument here for non-line comments, and personally I cannot remember the last time Ive used one. Comments in and of themselves are decreasing in frequency, and should be focused on explaining why code is doing stuff. That is done using line comments. Leaving commented code in your codebase is an anti-best practice with the ubiquity of source control systems (and distributed systems make it even easier). Multi line comments arent going to make or break your programming language. Were not using stone knives and bearskins anymore. When I first started, not a few programmers used vi (not even vim) because things like syntax highlighting were a crutch. Programmers use tools, take advantage of that.»\n",
            "[4] «If the code is trivial, it doesnt need an explanatory comment. If the code is non-trivial, the explanatory comment will most likely also be non-trivial. Now, the trouble with non-trivial natural language is that many of us are not very good at reading it or writing it. Im sure your written communication skills are excellent, but nevertheless someone with a lesser grasp of written language might misunderstand your words. If you try very hard to write natural language that cannot be misinterpreted you end up with something like a legal document (and as we all know those are more verbose and difficult to understand than code). Code should be the most concise description of your logic, and there shouldnt be much debate about the meaning of your code because your compiler and platform have the final say. Personally I wouldnt say that you should never write a comment. Only that you should consider why your code needs a comment, and how you might fix that. This seems to be a common theme in answers here.»\n",
            "[5] «The principal purpose of code is commanding a computer to do something, so a good comment is never a substitute for good code because comments cant be executed. That being said, comments in the source are one form of documentation for other programmers (including yourself). If the comments are about more abstract issues than what the code is doing at every step, youre doing better than average. That level of abstraction varies with the tool youre using. Comments accompanying assembly language routines generally have a lower level of abstraction than, for example, this APL A←0⋄A⊣{2⊤⍵:1+3×⍵⋄⍵÷2}⍣{⍺=A+←1}⎕. I think that would probably merit a comment about the problem its intended to solve, hmmm?»\n",
            "\n",
            "[[ ## question ## ]]\n",
            "coding standard for clarity: comment every line of code?\n",
            "\n",
            "\n",
            "\u001b[31mAssistant message:\u001b[0m\n",
            "\n",
            "[[ ## reasoning ## ]]\n",
            "The context discusses the debate around commenting code, emphasizing that while clear and understandable code is essential, excessive commenting—such as commenting every line—can lead to redundancy and confusion. It suggests that comments should be reserved for explaining the purpose of larger code blocks rather than individual lines. The consensus is that code should be self-explanatory, and comments should only be used when necessary to clarify complex logic or provide context. This aligns with the idea that good coding practices prioritize readability and maintainability without burdening developers with unnecessary comments.\n",
            "\n",
            "[[ ## response ## ]]\n",
            "No, the coding standard should not advocate for commenting every line of code. Instead, comments should be used selectively to explain the purpose of larger code blocks or complex logic. Excessive commenting can lead to redundancy and may distract from the actual code, making it harder for developers to focus on the logic itself. The goal should be to write clear and understandable code that minimizes the need for comments, reserving them for important clarifications when necessary.\n",
            "\n",
            "\n",
            "\u001b[31mUser message:\u001b[0m\n",
            "\n",
            "[[ ## context ## ]]\n",
            "[1] «If you hold command and quickly tap tab you will cycle between your two most recently used applications without bringing up the heads up display. If you want to use a third party app Witch has options to ignore minimized windows.»\n",
            "[2] «Try this: On your Mac, Navigate to System Preferences Go to Mission Control Uncheck When switching to an application, switch to a Space with open windows for the application Try using the cmd+tab now.»\n",
            "[3] «Command + Tab until you get the apps icon. Before releasing the Command key, press and hold the Option key. You must switch to another app and let it take focus first. In other words, you cant just Command + Tab to another app and before actually selecting that app (by releasing the Command and Tab keys), switch right back to your minimized app, which you might attempt to do if you minimized it by accident or just simply changed your mind shortly after minimizing. Both the Command and left Option keys must be pressed on the same side (left or right) of the keyboard.»\n",
            "[4] «Command-option-click does not hide all open windows. As stated in the linked Apple article, this command hides all other open items leaving the clicked item visible. It is possible to hide all other items and minimize your most recently item with command+option+h+m. This is a combination of command+option+h - Hide Others and command+option+m - Minimize.»\n",
            "[5] «Cmd+` of course will work but I do it in different way. I do the Cmd+tab when i do that, I dont release the Cmd so the opened apps are still visible. While it is visible, I will press ▼. This way it will show all your opened application with animated displayed so you can navigate using arrow keys.»\n",
            "\n",
            "[[ ## question ## ]]\n",
            "cmd+tab does not work on hidden or minimized windows\n",
            "\n",
            "Respond with the corresponding output fields, starting with the field `[[ ## reasoning ## ]]`, then `[[ ## response ## ]]`, and then ending with the marker for `[[ ## completed ## ]]`.\n",
            "\n",
            "\n",
            "\u001b[31mResponse:\u001b[0m\n",
            "\n",
            "\u001b[32m[[ ## reasoning ## ]]\n",
            "The context provides various insights into how the Command + Tab functionality works on Mac systems, particularly regarding switching between applications. It highlights that Command + Tab allows users to cycle through their most recently used applications, but it does not activate minimized or hidden windows unless specific conditions are met. For instance, if an application is minimized, the user must first switch to another application and let it take focus before returning to the minimized app. This indicates that Command + Tab does not directly interact with minimized windows, which aligns with the user's observation that it does not work on hidden or minimized windows.\n",
            "\n",
            "[[ ## response ## ]]\n",
            "The Command + Tab functionality on Mac does not work with hidden or minimized windows directly. When you use Command + Tab, it allows you to cycle through your most recently used applications, but if an application is minimized, you cannot switch back to it using this shortcut unless you first switch to another application and let it take focus. This means that minimized windows will not be activated by Command + Tab until they are restored manually or until you switch focus away from them. If you want to manage minimized windows, you may need to use other shortcuts or methods, such as Command + Option + H + M to hide others and minimize the most recent item. \n",
            "\n",
            "[[ ## completed ## ]]\u001b[0m\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dspy.inspect_history(n=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "c27b2f5d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average Metric: 183.67 / 300 (61.2%): 100%|██████████| 300/300 [02:44<00:00,  1.82it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026/02/13 14:00:57 INFO dspy.evaluate.evaluate: Average Metric: 183.66667203555758 / 300 (61.2%)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>example_response</th>\n",
              "      <th>gold_doc_ids</th>\n",
              "      <th>reasoning</th>\n",
              "      <th>pred_response</th>\n",
              "      <th>SemanticF1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>when to use c over c++, and c++ over c?</td>\n",
              "      <td>If you are equally familiar with both C++ and C, it's advisable to...</td>\n",
              "      <td>[733]</td>\n",
              "      <td>The context provides insights into the strengths and weaknesses of...</td>\n",
              "      <td>Use C over C++ when working on embedded systems or projects where ...</td>\n",
              "      <td>✔️ [0.545]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>should images be stored in a git repository?</td>\n",
              "      <td>One viewpoint expresses that there is no significant downside, esp...</td>\n",
              "      <td>[6253, 6254, 6275, 6278, 8215]</td>\n",
              "      <td>The context presents various perspectives on whether images should...</td>\n",
              "      <td>Storing images in a Git repository is generally not recommended du...</td>\n",
              "      <td>✔️ [0.421]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       question  \\\n",
              "0       when to use c over c++, and c++ over c?   \n",
              "1  should images be stored in a git repository?   \n",
              "\n",
              "                                                        example_response  \\\n",
              "0  If you are equally familiar with both C++ and C, it's advisable to...   \n",
              "1  One viewpoint expresses that there is no significant downside, esp...   \n",
              "\n",
              "                     gold_doc_ids  \\\n",
              "0                           [733]   \n",
              "1  [6253, 6254, 6275, 6278, 8215]   \n",
              "\n",
              "                                                               reasoning  \\\n",
              "0  The context provides insights into the strengths and weaknesses of...   \n",
              "1  The context presents various perspectives on whether images should...   \n",
              "\n",
              "                                                           pred_response  \\\n",
              "0  Use C over C++ when working on embedded systems or projects where ...   \n",
              "1  Storing images in a Git repository is generally not recommended du...   \n",
              "\n",
              "   SemanticF1  \n",
              "0  ✔️ [0.545]  \n",
              "1  ✔️ [0.421]  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "            <div style='\n",
              "                text-align: center;\n",
              "                font-size: 16px;\n",
              "                font-weight: bold;\n",
              "                color: #555;\n",
              "                margin: 10px 0;'>\n",
              "                ... 298 more rows not displayed ...\n",
              "            </div>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "61.22"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "evaluate(optimized_rag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1f9b370f",
      "metadata": {},
      "outputs": [],
      "source": [
        "cost = sum([x['cost'] for x in lm.history if x['cost'] is not None])  # in USD, as calculated by LiteLLM for certain providers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "0792575c",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1.8327576000000048"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b2c7fe8b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Prediction(\n",
              "    reasoning=\"The context provides various insights into how the Command + Tab functionality works on Mac systems, particularly regarding switching between applications. It highlights that Command + Tab allows users to cycle through their most recently used applications, but it does not activate minimized or hidden windows unless specific conditions are met. For instance, if an application is minimized, the user must first switch to another application and let it take focus before returning to the minimized app. This indicates that Command + Tab does not directly interact with minimized windows, which aligns with the user's observation that it does not work on hidden or minimized windows.\",\n",
              "    response='The Command + Tab functionality on Mac does not work with hidden or minimized windows directly. When you use Command + Tab, it allows you to cycle through your most recently used applications, but if an application is minimized, you cannot switch back to it using this shortcut unless you first switch to another application and let it take focus. This means that minimized windows will not be activated by Command + Tab until they are restored manually or until you switch focus away from them. If you want to manage minimized windows, you may need to use other shortcuts or methods, such as Command + Option + H + M to hide others and minimize the most recent item.'\n",
              ")"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "optimized_rag.save(\"optimized_rag.json\")\n",
        "\n",
        "loaded_rag = RAG()\n",
        "loaded_rag.load(\"optimized_rag.json\")\n",
        "\n",
        "loaded_rag(question=\"cmd+tab does not work on hidden or minimized windows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "df190f5f",
      "metadata": {},
      "outputs": [],
      "source": [
        "rag.save(\"rag.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b1e3e91",
      "metadata": {},
      "source": [
        "Take a look at the prompts before vs after!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "dspy-venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
